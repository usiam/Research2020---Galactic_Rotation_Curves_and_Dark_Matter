{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mangaHIall = Table.read('input/mangaHIall_WF50_positive.txt', format = 'ascii.commented_header')\n",
    "mangaHIall_walls = Table.read('input/mangaHIall_walls_WF50_positive.txt', format = 'ascii.commented_header')\n",
    "mangaHIall_voids = Table.read('input/mangaHIall_voids_WF50_positive.txt', format = 'ascii.commented_header')\n",
    "\n",
    "mangaHI = Table.read(\"out_txt/GBT_HI_with_MStar.txt\", format=\"ascii.commented_header\")\n",
    "alfaHI = Table.read(\"out_txt/ALFALFA_HI_with_MStar.txt\", format=\"ascii.commented_header\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HI correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HI_correction(data):\n",
    "    #adds a HI column from logHI column\n",
    "    data['HI'] = np.ones(len(data), dtype = np.float64)\n",
    "    data['HI'] = 10**(data['logHI'])\n",
    "    \n",
    "    c = 1/(data['NSA_ba'])**0.12\n",
    "    data['HI_corrected'] = np.ones(len(data), dtype = np.float64)\n",
    "    data['HI_corrected'] = c * data['HI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "HI_correction(mangaHIall)\n",
    "HI_correction(mangaHIall_walls)\n",
    "HI_correction(mangaHIall_voids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding SNR data to mangaHIall, mangaHIall_walls, and mangaHIall_voids tables from mangaHI and alfaHI tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_SNR_col(data):\n",
    "    \n",
    "    data['SNR'] = np.NaN * np.ones(len(data), dtype = np.float64)\n",
    "\n",
    "    for i in range(len(mangaHI)):\n",
    "        for j in range(len(data)):\n",
    "            if (data['MaNGA_plate'][j], data['MaNGA_IFU'][j]) == (mangaHI['Plate'][i], mangaHI['IFU'][i]):\n",
    "                data['SNR'][j] = mangaHI['snr'][i]\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "    for i in range(len(alfaHI)):\n",
    "        for j in range(len(data)):\n",
    "            if (data['MaNGA_plate'][j], data['MaNGA_IFU'][j]) == (alfaHI['Plate'][i], alfaHI['IFU'][i]):\n",
    "                data['SNR'][j] = alfaHI['SNR'][i]\n",
    "    \n",
    "    \n",
    "        \n",
    "#     plate_boolean = mangaHIall['MaNGA_plate'] == mangaHI['Plate']\n",
    "#     ifu_boolean = mangaHIall['MaNGA_IFU'] == mangaHI['IFU']\n",
    "#     match_boolean = np.logical_and(plate_boolean, ifu_boolean)    \n",
    "    \n",
    "#     plate_boolean = mangaHIall['MaNGA_plate'] == alfaHI['Plate']\n",
    "#     ifu_boolean = mangaHIall['MaNGA_IFU'] == alfaHI['IFU']\n",
    "#     match_boolean = np.logical_and(plate_boolean, ifu_boolean) \n",
    "#     mangaHIall['SNR'][match_boolean] = alfaHI['SNR'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_SNR_col(mangaHIall)\n",
    "add_SNR_col(mangaHIall_walls)\n",
    "add_SNR_col(mangaHIall_voids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Inclination Angle to mangaHIall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Inclination_Angle(data):\n",
    "    data['Inc_Angle'] = np.ones(len(data), dtype = np.float64)\n",
    "\n",
    "    q=0.2 #intrinsic disc thickness\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if data['NSA_ba'][i] >= 0.200:\n",
    "            data['Inc_Angle'][i] = (np.arccos(np.sqrt((data['NSA_ba'][i]**2 - q**2)/(1-q**2))))\n",
    "        else:\n",
    "            data['Inc_Angle'][i] = (np.arccos(np.sqrt((data['NSA_ba'][i]**2))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_Inclination_Angle(mangaHIall)\n",
    "add_Inclination_Angle(mangaHIall_walls)\n",
    "add_Inclination_Angle(mangaHIall_voids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocity correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vel_correction(data):\n",
    "    \n",
    "    data['WF50_corrected'] = np.ones(len(data), dtype = np.float64)\n",
    "    data['WP20_corrected'] = np.ones(len(data), dtype = np.float64)\n",
    "    data['logSNR'] = np.ones(len(data), dtype = np.float64)\n",
    "    data['logSNR'] = np.log10(data['SNR'])\n",
    "\n",
    "    v = 5.00 #km/s\n",
    "    t = 6.5 #km/s,\n",
    "    l = 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if data['Inc_Angle'][i] == 0 or data['Inc_Angle'][i] == np.pi:\n",
    "            data['Inc_Angle'][i] = np.pi/2\n",
    "        else:\n",
    "            if 0<data['logSNR'][i]<0.6:\n",
    "                l = 0.05\n",
    "            elif 0.6<data['logSNR'][i]<1.1:\n",
    "                l = -0.4685 + 0.785*data['logSNR'][i]\n",
    "            elif data['logSNR'][i]>1.1:\n",
    "                l = 0.395          \n",
    "        WF50_corr = (((data['WF50'][i] - (2*v*l))/(1+data['NSA_redshift'][i])) - t) * (1/np.sin(data['Inc_Angle'][i]))\n",
    "        WP20_corr = (((data['WP20'][i] - (2*v*l))/(1+data['NSA_redshift'][i])) - t) * (1/np.sin(data['Inc_Angle'][i]))\n",
    "        data['WF50_corrected'][i] = WF50_corr\n",
    "        data['WP20_corrected'][i] = WP20_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_correction(mangaHIall)\n",
    "vel_correction(mangaHIall_walls)\n",
    "vel_correction(mangaHIall_voids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing corrected files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mangaHIall.write('input/mangaHIall_WF50_positive_corrected.txt', format = 'ascii.commented_header')\n",
    "mangaHIall_walls.write('input/mangaHIall_walls_WF50_positive_corrected.txt', format = 'ascii.commented_header')\n",
    "mangaHIall_voids.write('input/mangaHIall_voids_WF50_positive_corrected.txt', format = 'ascii.commented_header')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
